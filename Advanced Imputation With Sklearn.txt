
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.base import TransformerMixin, BaseEstimator

are these used for making imputation in data? what does it different from using the normal fillna() method? also can you give example of using those libraries please?

------------------------------------------------

Yes, these libraries are used for data imputation, but they offer more sophisticated capabilities compared to the simple fillna() method. Let me explain the key differences and provide examples:

1. Main Differences from fillna():
- More statistical imputation strategies (mean, median, most_frequent, constant)
- Automatically handles future data through fit/transform pattern
- Can be integrated into scikit-learn pipelines
- Preserves data types and handles multiple columns simultaneously
- Can specify different strategies for different column types

Let me show you examples of both approaches:

```python
# Basic fillna() approach
import pandas as pd

df = pd.DataFrame({
    'age': [25, np.nan, 30, np.nan],
    'salary': [50000, 60000, np.nan, 55000],
    'category': ['A', None, 'B', 'A']
})

# Simple fillna()
df['age'].fillna(df['age'].mean())
```

Now, using sklearn imputers:

```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# Create sample data
df = pd.DataFrame({
    'age': [25, np.nan, 30, np.nan],
    'salary': [50000, 60000, np.nan, 55000],
    'category': ['A', None, 'B', 'A']
})

# Numeric imputer
num_imputer = SimpleImputer(strategy='mean')
# Categorical imputer
cat_imputer = SimpleImputer(strategy='most_frequent')

# Fit and transform numeric columns
df[['age', 'salary']] = num_imputer.fit_transform(df[['age', 'salary']])
# Fit and transform categorical column
df[['category']] = cat_imputer.fit_transform(df[['category']])
```

For more complex scenarios, you can use ColumnTransformer:

```python
# More advanced approach using ColumnTransformer
numeric_features = ['age', 'salary']
categorical_features = ['category']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='mean'), numeric_features),
        ('cat', SimpleImputer(strategy='most_frequent'), categorical_features)
    ])

# Fit and transform all columns at once
imputed_data = preprocessor.fit_transform(df)
```

You can even create custom imputers:

```python
class CustomImputer(BaseEstimator, TransformerMixin):
    def __init__(self, custom_value=0):
        self.custom_value = custom_value
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        X_copy = X.copy()
        # Custom imputation logic
        X_copy[X_copy['age'].isna()] = self.custom_value
        return X_copy

# Usage
custom_imputer = CustomImputer(custom_value=25)
```

Key advantages of using sklearn imputers:

1. Pipeline Integration:
```python
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

pipeline = Pipeline([
    ('imputer', preprocessor),
    ('classifier', RandomForestClassifier())
])

# Can be used in cross-validation
pipeline.fit(X_train, y_train)
pipeline.predict(X_test)
```

2. Different Strategies:
```python
# Mean imputation
mean_imputer = SimpleImputer(strategy='mean')

# Median imputation
median_imputer = SimpleImputer(strategy='median')

# Most frequent value
mode_imputer = SimpleImputer(strategy='most_frequent')

# Constant value
constant_imputer = SimpleImputer(strategy='constant', fill_value=-999)
```

3. Handling New Data:
```python
# Fit on training data
imputer.fit(X_train)

# Can be applied to new data using same parameters
X_test_imputed = imputer.transform(X_test)
```

The sklearn approach is particularly useful when:
- You need to ensure consistent imputation between training and test sets
- You're building a machine learning pipeline
- You need different imputation strategies for different types of features
- You want to ensure reproducibility of your imputation strategy
- You're dealing with large datasets and need efficient processing

fillna() is simpler and might be sufficient for basic data cleaning, but sklearn imputers provide more robust and maintainable solutions for machine learning workflows.