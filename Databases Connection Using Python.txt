Source: https://claude.ai/chat/297b57c9-8c8c-46ae-b6dc-10809da7e3bb

---------------------------------------------------------------------------------------------------

can you make for several database connection from python? make example for as many as database as you can including the famous ones and data warehouse, the code should works similar to this simple in postgres, also include the postgres one in your response to make the list complete

import psycopg2
import pandas as pd
conn = psycopg2.connect(
    host="localhost",
    port="5432",
    dbname="your_db",
    user="your_user",
    password="your_password"
)
df = pd.read_sql_query("SELECT total_spend, years_as_customer, customer_id FROM transaction_data", conn)
conn.close()

---------------------------------------------------------------------------------------------------

"""
Database Connection Examples in Python
This file contains examples of connecting to various databases and data warehouses from Python.
Each example demonstrates establishing a connection, executing a query, and loading data into a pandas DataFrame.
"""

import pandas as pd

#---------------------------------------------
# PostgreSQL
#---------------------------------------------
def connect_postgresql():
    import psycopg2
    
    conn = psycopg2.connect(
        host="localhost",
        port="5432",
        dbname="your_db",
        user="your_user",
        password="your_password"
    )
    
    df = pd.read_sql_query("SELECT total_spend, years_as_customer, customer_id FROM transaction_data", conn)
    conn.close()
    return df

#---------------------------------------------
# MySQL
#---------------------------------------------
def connect_mysql():
    import mysql.connector
    
    conn = mysql.connector.connect(
        host="localhost",
        port=3306,
        database="your_db",
        user="your_user",
        password="your_password"
    )
    
    df = pd.read_sql_query("SELECT * FROM customers", conn)
    conn.close()
    return df

# Alternative MySQL connection using PyMySQL
def connect_mysql_pymysql():
    import pymysql
    
    conn = pymysql.connect(
        host="localhost",
        port=3306,
        database="your_db",
        user="your_user",
        password="your_password"
    )
    
    df = pd.read_sql_query("SELECT * FROM customers", conn)
    conn.close()
    return df

#---------------------------------------------
# SQLite
#---------------------------------------------
def connect_sqlite():
    import sqlite3
    
    conn = sqlite3.connect("path/to/your_database.db")
    
    df = pd.read_sql_query("SELECT * FROM users", conn)
    conn.close()
    return df

#---------------------------------------------
# Microsoft SQL Server
#---------------------------------------------
def connect_mssql():
    import pyodbc
    
    conn = pyodbc.connect(
        "DRIVER={ODBC Driver 17 for SQL Server};"
        "SERVER=your_server_name;"
        "DATABASE=your_db;"
        "UID=your_user;"
        "PWD=your_password"
    )
    
    # Alternative connection string format
    # conn = pyodbc.connect(
    #     "Driver={SQL Server};"
    #     "Server=your_server_name,port;"
    #     "Database=your_db;"
    #     "Trusted_Connection=yes;"  # For Windows authentication
    # )
    
    df = pd.read_sql_query("SELECT * FROM sales", conn)
    conn.close()
    return df

# Alternative SQL Server connection using pymssql
def connect_mssql_pymssql():
    import pymssql
    
    conn = pymssql.connect(
        server="your_server_name",
        database="your_db",
        user="your_user",
        password="your_password"
    )
    
    df = pd.read_sql_query("SELECT * FROM sales", conn)
    conn.close()
    return df

#---------------------------------------------
# Oracle
#---------------------------------------------
def connect_oracle():
    import cx_Oracle
    
    # Set Oracle Client library path if needed
    # cx_Oracle.init_oracle_client(lib_dir="/path/to/oracle/instantclient")
    
    conn = cx_Oracle.connect(
        user="your_user",
        password="your_password",
        dsn="your_host:1521/your_service_name"
    )
    
    # Alternative connection format
    # dsn = cx_Oracle.makedsn(
    #     host="your_host",
    #     port=1521,
    #     service_name="your_service_name"
    # )
    # conn = cx_Oracle.connect(user="your_user", password="your_password", dsn=dsn)
    
    df = pd.read_sql_query("SELECT * FROM orders", conn)
    conn.close()
    return df

#---------------------------------------------
# MongoDB (NoSQL)
#---------------------------------------------
def connect_mongodb():
    from pymongo import MongoClient
    import json
    from bson.json_util import dumps
    
    client = MongoClient(
        host="localhost",
        port=27017,
        username="your_user",
        password="your_password"
    )
    
    # Alternative connection string format
    # client = MongoClient("mongodb://your_user:your_password@localhost:27017/")
    
    db = client["your_db"]
    collection = db["your_collection"]
    
    # Query MongoDB and convert to DataFrame
    cursor = collection.find({})
    documents = list(cursor)
    json_data = dumps(documents)
    data = json.loads(json_data)
    df = pd.DataFrame(data)
    
    client.close()
    return df

#---------------------------------------------
# Amazon Redshift
#---------------------------------------------
def connect_redshift():
    # Method 1: Using psycopg2 (Redshift is PostgreSQL-compatible)
    import psycopg2
    
    conn = psycopg2.connect(
        host="redshift-cluster.example.region.redshift.amazonaws.com",
        port=5439,
        dbname="your_db",
        user="your_user",
        password="your_password"
    )
    
    # Method 2: Using SQLAlchemy with redshift_connector
    # import redshift_connector
    # 
    # conn = redshift_connector.connect(
    #     host="redshift-cluster.example.region.redshift.amazonaws.com",
    #     port=5439,
    #     database="your_db",
    #     user="your_user",
    #     password="your_password"
    # )
    
    df = pd.read_sql_query("SELECT * FROM customer_analytics", conn)
    conn.close()
    return df

#---------------------------------------------
# Google BigQuery
#---------------------------------------------
def connect_bigquery():
    from google.cloud import bigquery
    
    # Authenticate using a service account key file
    # Set environment variable: os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "path/to/key.json"
    # Or authenticate explicitly:
    client = bigquery.Client(project="your-project-id")
    
    query = """
    SELECT * FROM `your-project-id.dataset_name.table_name`
    LIMIT 1000
    """
    
    df = client.query(query).to_dataframe()
    return df

#---------------------------------------------
# Snowflake
#---------------------------------------------
def connect_snowflake():
    import snowflake.connector
    from snowflake.connector.pandas_tools import pd_writer
    
    conn = snowflake.connector.connect(
        account="your_account",
        user="your_user",
        password="your_password",
        warehouse="your_warehouse",
        database="your_database",
        schema="your_schema",
        role="your_role"       # Optional
    )
    
    # Using cursor to execute query
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM sales_data")
    df = cursor.fetch_pandas_all()
    cursor.close()
    
    # Alternative direct approach with pandas
    # df = pd.read_sql_query("SELECT * FROM sales_data", conn)
    
    conn.close()
    return df

#---------------------------------------------
# Azure Synapse Analytics (formerly SQL Data Warehouse)
#---------------------------------------------
def connect_synapse():
    import pyodbc
    
    conn = pyodbc.connect(
        "Driver={ODBC Driver 17 for SQL Server};"
        "Server=your-server.sql.azuresynapse.net;"
        "Database=your_db;"
        "UID=your_user;"
        "PWD=your_password;"
        "Encrypt=yes;"
        "TrustServerCertificate=no;"
        "Connection Timeout=30;"
    )
    
    df = pd.read_sql_query("SELECT * FROM fact_sales", conn)
    conn.close()
    return df

#---------------------------------------------
# Amazon Athena
#---------------------------------------------
def connect_athena():
    import boto3
    import time
    import io
    
    athena_client = boto3.client(
        'athena',
        region_name='us-east-1',
        aws_access_key_id='your_access_key',
        aws_secret_access_key='your_secret_key'
    )
    
    # S3 location to store query results
    s3_output = 's3://your-bucket/path/to/query/results/'
    
    # Execute query
    query = "SELECT * FROM your_database.your_table LIMIT 1000"
    response = athena_client.start_query_execution(
        QueryString=query,
        QueryExecutionContext={'Database': 'your_database'},
        ResultConfiguration={'OutputLocation': s3_output}
    )
    
    # Get query ID
    query_execution_id = response['QueryExecutionId']
    
    # Wait for query to complete
    state = 'RUNNING'
    while state in ['RUNNING', 'QUEUED']:
        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)
        state = response['QueryExecution']['Status']['State']
        if state in ['RUNNING', 'QUEUED']:
            time.sleep(1)
    
    # Get results
    if state == 'SUCCEEDED':
        s3_client = boto3.client('s3')
        bucket = s3_output.split('/')[2]
        key = '/'.join(s3_output.split('/')[3:]) + query_execution_id + '.csv'
        
        response = s3_client.get_object(Bucket=bucket, Key=key)
        csv_content = response['Body'].read().decode('utf-8')
        df = pd.read_csv(io.StringIO(csv_content))
        return df
    else:
        error_message = response['QueryExecution']['Status']['StateChangeReason']
        raise Exception(f"Athena query failed: {error_message}")

#---------------------------------------------
# Apache Cassandra
#---------------------------------------------
def connect_cassandra():
    from cassandra.cluster import Cluster
    from cassandra.auth import PlainTextAuthProvider
    
    auth_provider = PlainTextAuthProvider(
        username='your_user',
        password='your_password'
    )
    
    cluster = Cluster(
        ['127.0.0.1'],  # List of contact points
        port=9042,
        auth_provider=auth_provider
    )
    
    session = cluster.connect('your_keyspace')
    
    # Execute CQL query
    rows = session.execute("SELECT * FROM your_table")
    
    # Convert to DataFrame
    df = pd.DataFrame(list(rows))
    
    cluster.shutdown()
    return df

#---------------------------------------------
# ClickHouse
#---------------------------------------------
def connect_clickhouse():
    from clickhouse_driver import Client
    
    client = Client(
        host='localhost',
        port=9000,
        user='your_user',
        password='your_password',
        database='your_db'
    )
    
    # Execute query and get result as DataFrame
    df = pd.DataFrame(
        client.execute("SELECT * FROM your_table"),
        columns=[col[0] for col in client.execute("DESCRIBE your_table")]
    )
    
    return df

#---------------------------------------------
# IBM Db2
#---------------------------------------------
def connect_db2():
    import ibm_db
    import ibm_db_dbi
    
    # Create a connection string
    conn_string = (
        "DATABASE=your_db;"
        "HOSTNAME=localhost;"
        "PORT=50000;"
        "PROTOCOL=TCPIP;"
        "UID=your_user;"
        "PWD=your_password;"
    )
    
    # Connect to Db2
    ibm_conn = ibm_db.connect(conn_string, "", "")
    conn = ibm_db_dbi.Connection(ibm_conn)
    
    df = pd.read_sql_query("SELECT * FROM employees", conn)
    conn.close()
    return df

#---------------------------------------------
# Teradata
#---------------------------------------------
def connect_teradata():
    import teradatasql
    
    conn = teradatasql.connect(
        host="your_teradata_server",
        user="your_user",
        password="your_password",
        encryptdata='true'
    )
    
    df = pd.read_sql_query("SELECT * FROM sales_data", conn)
    conn.close()
    return df

#---------------------------------------------
# Apache Hive
#---------------------------------------------
def connect_hive():
    from pyhive import hive
    
    conn = hive.Connection(
        host="your_hive_server",
        port=10000,
        username="your_user",
        password="your_password",
        database="default"
    )
    
    df = pd.read_sql_query("SELECT * FROM customer_data", conn)
    conn.close()
    return df

#---------------------------------------------
# Presto/Trino
#---------------------------------------------
def connect_presto():
    import prestodb
    
    conn = prestodb.dbapi.connect(
        host="localhost",
        port=8080,
        user="your_user",
        catalog="hive",
        schema="default"
    )
    
    df = pd.read_sql_query("SELECT * FROM orders", conn)
    conn.close()
    return df

# Trino (formerly PrestoSQL) connection
def connect_trino():
    import trino
    
    conn = trino.dbapi.connect(
        host="localhost",
        port=8080,
        user="your_user",
        catalog="hive",
        schema="default"
    )
    
    df = pd.read_sql_query("SELECT * FROM orders", conn)
    conn.close()
    return df

#---------------------------------------------
# SQLAlchemy Example (Unified API for multiple databases)
#---------------------------------------------
def connect_sqlalchemy(db_type):
    from sqlalchemy import create_engine
    
    connection_strings = {
        'postgresql': 'postgresql://your_user:your_password@localhost:5432/your_db',
        'mysql': 'mysql+pymysql://your_user:your_password@localhost:3306/your_db',
        'sqlite': 'sqlite:///path/to/your_database.db',
        'mssql': 'mssql+pyodbc://your_user:your_password@your_server/your_db?driver=ODBC+Driver+17+for+SQL+Server',
        'oracle': 'oracle+cx_oracle://your_user:your_password@your_host:1521/?service_name=your_service',
        'snowflake': 'snowflake://your_user:your_password@your_account/your_database/your_schema?warehouse=your_warehouse',
        'redshift': 'redshift+psycopg2://your_user:your_password@your_cluster:5439/your_db',
        'bigquery': 'bigquery://your-project-id'
    }
    
    if db_type not in connection_strings:
        raise ValueError(f"Unsupported database type: {db_type}")
    
    engine = create_engine(connection_strings[db_type])
    df = pd.read_sql("SELECT * FROM your_table", engine)
    engine.dispose()
    return df